{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install linearmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Card and Krueger (1994) DiD Analysis in Python\n",
    "\n",
    "This notebook replicates the Stata analysis performed on the Card and Krueger dataset regarding the impact of a minimum wage increase in New Jersey on fast-food employment, using Pennsylvania as a control group. We will perform Difference-in-Differences (DiD) estimation using various methods.\n",
    "\n",
    "**Dataset:** `Card_Krueger_1994.csv` \n",
    "**Variables:**\n",
    "*   `sheet`: Unique store identifier\n",
    "*   `fte`: Full-time equivalent employment\n",
    "*   `nj`: Dummy variable, 1 if the restaurant is in New Jersey, 0 if in Pennsylvania\n",
    "*   `after`: Dummy variable, 1 for the period after the minimum wage increase, 0 for before\n",
    "*   `njafter`: Interaction term (`nj` * `after`), 1 for NJ restaurants after the increase\n",
    "*   `dfte`: Change in `fte` between the 'before' and 'after' periods (`fte_after - fte_before`) for each restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Import Libraries and Load Data\n",
    "\n",
    "First, we need to import the necessary Python libraries and load the dataset (`Did.csv`) into a pandas DataFrame. We'll also display the first few rows and some basic information about the data to ensure it's loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from: Card_Krueger_1994.csv\n",
      "\n",
      "First 5 rows of the dataframe:\n",
      "   sheet    fte    nj  after  njafter   dfte\n",
      "0      1 31.000 1.000  0.000    0.000    NaN\n",
      "1      1 40.000 1.000  1.000    1.000  9.000\n",
      "2      2 13.000 1.000  0.000    0.000    NaN\n",
      "3      2 12.500 1.000  1.000    1.000 -0.500\n",
      "4      3 12.500 1.000  0.000    0.000    NaN\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 698 entries, 0 to 697\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   sheet    698 non-null    int64  \n",
      " 1   fte      698 non-null    float64\n",
      " 2   nj       698 non-null    float64\n",
      " 3   after    698 non-null    float64\n",
      " 4   njafter  698 non-null    float64\n",
      " 5   dfte     349 non-null    float64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 32.8 KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "        sheet     fte      nj   after  njafter    dfte\n",
      "count 698.000 698.000 698.000 698.000  698.000 349.000\n",
      "mean  245.946  17.784   0.814   0.500    0.407  -0.151\n",
      "std   149.172   8.985   0.390   0.500    0.492   8.606\n",
      "min     1.000   3.000   0.000   0.000    0.000 -43.500\n",
      "25%   118.000  11.500   1.000   0.000    0.000  -3.750\n",
      "50%   233.000  16.750   1.000   0.500    0.000   0.000\n",
      "75%   376.000  22.000   1.000   1.000    1.000   4.250\n",
      "max   522.000  80.000   1.000   1.000    1.000  26.000\n",
      "\n",
      "Missing values per column:\n",
      "sheet        0\n",
      "fte          0\n",
      "nj           0\n",
      "after        0\n",
      "njafter      0\n",
      "dfte       349\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "sheet        int64\n",
      "fte        float64\n",
      "nj         float64\n",
      "after      float64\n",
      "njafter    float64\n",
      "dfte       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "from linearmodels.panel import PanelOLS\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the file path \n",
    "# Make sure 'Card_Krueger_1994.csv' is in your current working directory or provide the full path\n",
    "file_path = \"Card_Krueger_1994.csv\" \n",
    "# Set display precision for floating-point numbers in pandas output\n",
    "pd.options.display.float_format = '{:.3f}'.format \n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded data from: {file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    print(\"Please make sure the CSV file is in the correct directory or update the 'file_path'.\")\n",
    "    # Exit or raise an error if the file isn't found, as subsequent steps depend on it.\n",
    "    # For this example, we'll proceed assuming the file loads, but in practice, handle this robustly.\n",
    "    df = None # Set df to None if loading fails\n",
    "\n",
    "# --- Initial Data Exploration (only if df is loaded) ---\n",
    "if df is not None:\n",
    "    print(\"\\nFirst 5 rows of the dataframe:\")\n",
    "    # Drop the unnamed index column if it exists from CSV saving/loading\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Ensure key dummy variables are appropriate types (e.g., float or int)\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "else:\n",
    "    print(\"\\nSkipping data exploration as the file could not be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataframe:\n",
      "   sheet    fte    nj  after  njafter   dfte\n",
      "0      1 31.000 1.000  0.000    0.000    NaN\n",
      "1      1 40.000 1.000  1.000    1.000  9.000\n",
      "2      2 13.000 1.000  0.000    0.000    NaN\n",
      "3      2 12.500 1.000  1.000    1.000 -0.500\n",
      "4      3 12.500 1.000  0.000    0.000    NaN\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 698 entries, 0 to 697\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   sheet    698 non-null    int64  \n",
      " 1   fte      698 non-null    float64\n",
      " 2   nj       698 non-null    float64\n",
      " 3   after    698 non-null    float64\n",
      " 4   njafter  698 non-null    float64\n",
      " 5   dfte     349 non-null    float64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 32.8 KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "        sheet     fte      nj   after  njafter    dfte\n",
      "count 698.000 698.000 698.000 698.000  698.000 349.000\n",
      "mean  245.946  17.784   0.814   0.500    0.407  -0.151\n",
      "std   149.172   8.985   0.390   0.500    0.492   8.606\n",
      "min     1.000   3.000   0.000   0.000    0.000 -43.500\n",
      "25%   118.000  11.500   1.000   0.000    0.000  -3.750\n",
      "50%   233.000  16.750   1.000   0.500    0.000   0.000\n",
      "75%   376.000  22.000   1.000   1.000    1.000   4.250\n",
      "max   522.000  80.000   1.000   1.000    1.000  26.000\n",
      "\n",
      "Data types:\n",
      "sheet        int64\n",
      "fte        float64\n",
      "nj         float64\n",
      "after      float64\n",
      "njafter    float64\n",
      "dfte       float64\n",
      "dtype: object\n",
      "\n",
      "Total observations: 698\n",
      "Number of unique stores (sheets): 349\n",
      "\n",
      "NaN counts after numeric conversion:\n",
      "sheet        0\n",
      "fte          0\n",
      "nj           0\n",
      "after        0\n",
      "njafter      0\n",
      "dfte       349\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "from linearmodels.panel import PanelOLS\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the file path (make sure 'Did.csv' is accessible)\n",
    "file_path = \"Card_Krueger_1994.csv\" \n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Optional: Drop the unnamed index column if it exists from CSV conversion\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# --- Initial Data Exploration ---\n",
    "print(\"First 5 rows of the dataframe:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "# Set display precision for better readability\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "print(df.describe())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nTotal observations: {len(df)}\")\n",
    "print(f\"Number of unique stores (sheets): {df['sheet'].nunique()}\")\n",
    "\n",
    "# Ensure relevant columns are numeric, handling potential non-numeric entries if necessary\n",
    "cols_to_numeric = ['fte', 'nj', 'after', 'njafter', 'dfte']\n",
    "for col in cols_to_numeric:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce') # 'coerce' turns errors into NaN\n",
    "\n",
    "# Check for NaNs introduced\n",
    "print(\"\\nNaN counts after numeric conversion:\")\n",
    "print(df.isnull().sum())\n",
    "# Consider how to handle NaNs if significant, e.g., df.dropna(subset=cols_to_numeric, inplace=True)\n",
    "# For this dataset based on the Stata output, we expect few NaNs except in dfte for the 'before' period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1: Use mean differences to compute the difference in means estimate\n",
    "\n",
    "**Instructions:**\n",
    "To calculate the difference-in-differences (DiD) estimate manually, we perform the following steps:\n",
    "1.  Calculate the average `fte` for each group (NJ and PA) in each time period (before and after).\n",
    "2.  Calculate the change in average `fte` over time for each group (PA's change and NJ's change).\n",
    "3.  Calculate the difference between these two changes. This is the DiD estimate.\n",
    "\n",
    "We will also perform t-tests to compare the means between NJ and PA within each time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Mean FTE Employment by State and Period ---\n",
      "    Before  After  Difference (After - Before)\n",
      "PA  20.300 18.254                       -2.046\n",
      "NJ  17.301 17.584                        0.283\n",
      "\n",
      "--- Summary Table (Difference-in-Differences) ---\n",
      "                      Before  After  Difference\n",
      "PA                    20.300 18.254      -2.046\n",
      "NJ                    17.301 17.584       0.283\n",
      "Difference (NJ - PA)  -2.999 -0.670       2.329\n",
      "\n",
      "Difference-in-Differences Estimate: 2.329\n",
      "\n",
      "--- T-Tests (Comparing NJ vs PA within each period) ---\n",
      "T-test Before (NJ vs PA): statistic=-2.279, pvalue=0.023\n",
      "  Mean PA Before: 20.300, Mean NJ Before: 17.301, Diff (NJ-PA): -2.999\n",
      "T-test After (NJ vs PA): statistic=-0.586, pvalue=0.558\n",
      "  Mean PA After: 18.254, Mean NJ After: 17.584, Diff (NJ-PA): -0.670\n",
      "\n",
      "Note: The significance of the DiD estimate itself is typically assessed via regression (see next steps).\n"
     ]
    }
   ],
   "source": [
    "# --- Manual Calculation of Means and DiD ---\n",
    "\n",
    "if df is not None:\n",
    "    # Separate data into before and after periods\n",
    "    df_before = df[df['after'] == 0].copy()\n",
    "    df_after = df[df['after'] == 1].copy()\n",
    "\n",
    "    # Calculate means for each group and time period using pivot_table\n",
    "    means_pivot = pd.pivot_table(df, values='fte', index='nj', columns='after', aggfunc='mean')\n",
    "    means_pivot.index = ['PA', 'NJ'] # Rename index (0=PA, 1=NJ)\n",
    "    means_pivot.columns = ['Before', 'After'] # Rename columns (0=Before, 1=After)\n",
    "\n",
    "    # Calculate differences over time for each state\n",
    "    means_pivot['Difference (After - Before)'] = means_pivot['After'] - means_pivot['Before']\n",
    "\n",
    "    # Calculate differences between states for each period\n",
    "    diff_before = means_pivot.loc['NJ', 'Before'] - means_pivot.loc['PA', 'Before']\n",
    "    diff_after = means_pivot.loc['NJ', 'After'] - means_pivot.loc['PA', 'After']\n",
    "\n",
    "    # Calculate the Difference-in-Differences estimate\n",
    "    # Difference of the time differences: (NJ_After - NJ_Before) - (PA_After - PA_Before)\n",
    "    did_estimate = means_pivot.loc['NJ', 'Difference (After - Before)'] - means_pivot.loc['PA', 'Difference (After - Before)']\n",
    "\n",
    "    # Create a summary table \n",
    "    summary_table = pd.DataFrame({\n",
    "        'Before': [means_pivot.loc['PA', 'Before'], means_pivot.loc['NJ', 'Before'], diff_before],\n",
    "        'After': [means_pivot.loc['PA', 'After'], means_pivot.loc['NJ', 'After'], diff_after],\n",
    "        'Difference': [means_pivot.loc['PA', 'Difference (After - Before)'], means_pivot.loc['NJ', 'Difference (After - Before)'], did_estimate]\n",
    "    }, index=['PA', 'NJ', 'Difference (NJ - PA)'])\n",
    "\n",
    "\n",
    "    print(\"--- Mean FTE Employment by State and Period ---\")\n",
    "    print(means_pivot)\n",
    "    print(\"\\n--- Summary Table (Difference-in-Differences) ---\")\n",
    "    print(summary_table)\n",
    "    print(f\"\\nDifference-in-Differences Estimate: {did_estimate:.3f}\")\n",
    "\n",
    "    # --- Optional: T-tests for Mean Comparisons ---\n",
    "    print(\"\\n--- T-Tests (Comparing NJ vs PA within each period) ---\")\n",
    "\n",
    "    # Before period: NJ (1) vs PA (0)\n",
    "    pa_before_fte = df_before[df_before['nj'] == 0]['fte']\n",
    "    nj_before_fte = df_before[df_before['nj'] == 1]['fte']\n",
    "    # Perform t-test assuming equal variances, like the default 'ttest' command often does\n",
    "    ttest_before = stats.ttest_ind(nj_before_fte, pa_before_fte, nan_policy='omit', equal_var=True) \n",
    "    print(f\"T-test Before (NJ vs PA): statistic={ttest_before.statistic:.3f}, pvalue={ttest_before.pvalue:.3f}\")\n",
    "    print(f\"  Mean PA Before: {pa_before_fte.mean():.3f}, Mean NJ Before: {nj_before_fte.mean():.3f}, Diff (NJ-PA): {nj_before_fte.mean() - pa_before_fte.mean():.3f}\")\n",
    "\n",
    "    # After period: NJ (1) vs PA (0)\n",
    "    pa_after_fte = df_after[df_after['nj'] == 0]['fte']\n",
    "    nj_after_fte = df_after[df_after['nj'] == 1]['fte']\n",
    "    ttest_after = stats.ttest_ind(nj_after_fte, pa_after_fte, nan_policy='omit', equal_var=True)\n",
    "    print(f\"T-test After (NJ vs PA): statistic={ttest_after.statistic:.3f}, pvalue={ttest_after.pvalue:.3f}\")\n",
    "    print(f\"  Mean PA After: {pa_after_fte.mean():.3f}, Mean NJ After: {nj_after_fte.mean():.3f}, Diff (NJ-PA): {nj_after_fte.mean() - pa_after_fte.mean():.3f}\")\n",
    "\n",
    "    print(\"\\nNote: The significance of the DiD estimate itself is typically assessed via regression (see next steps).\")\n",
    "else:\n",
    "    print(\"Skipping Question 1 as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Estimate the difference-in-difference using a regression model in differences\n",
    "\n",
    "**Instructions:**\n",
    "An alternative way to estimate the DiD effect is to use a regression model where the dependent variable is the *change* in the outcome (`dfte`). We regress this change on the treatment indicator (`nj`) and the interaction term (`njafter`).\n",
    "\n",
    "This regression should be run on a dataset where each row represents one restaurant, using the calculated change `dfte`. This usually means using only the observations from the 'after' period, as `dfte` represents the change leading up to that point. We will estimate this model using Ordinary Least Squares (OLS) first with standard errors, and then with robust standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of observations for difference regression: 349\n",
      "\n",
      "--- Regression in Differences (OLS) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   dfte   R-squared:                       0.011\n",
      "Model:                            OLS   Adj. R-squared:                  0.008\n",
      "Method:                 Least Squares   F-statistic:                     3.905\n",
      "Date:                Tue, 01 Apr 2025   Prob (F-statistic):             0.0489\n",
      "Time:                        17:44:07   Log-Likelihood:                -1244.0\n",
      "No. Observations:                 349   AIC:                             2492.\n",
      "Df Residuals:                     347   BIC:                             2500.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -2.0462      1.063     -1.925      0.055      -4.137       0.045\n",
      "njafter        1.1644      0.589      1.976      0.049       0.006       2.323\n",
      "nj             1.1644      0.589      1.976      0.049       0.006       2.323\n",
      "==============================================================================\n",
      "Omnibus:                       49.343   Durbin-Watson:                   2.023\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              145.751\n",
      "Skew:                          -0.629   Prob(JB):                     2.24e-32\n",
      "Kurtosis:                       5.905   Cond. No.                     7.15e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.71e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "--- Regression in Differences (Robust SE - HC1) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   dfte   R-squared:                       0.011\n",
      "Model:                            OLS   Adj. R-squared:                  0.008\n",
      "Method:                 Least Squares   F-statistic:                     2.508\n",
      "Date:                Tue, 01 Apr 2025   Prob (F-statistic):              0.114\n",
      "Time:                        17:44:07   Log-Likelihood:                -1244.0\n",
      "No. Observations:                 349   AIC:                             2492.\n",
      "Df Residuals:                     347   BIC:                             2500.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -2.0462      1.395     -1.467      0.142      -4.780       0.688\n",
      "njafter        1.1644      0.735      1.584      0.113      -0.277       2.605\n",
      "nj             1.1644      0.735      1.584      0.113      -0.277       2.605\n",
      "==============================================================================\n",
      "Omnibus:                       49.343   Durbin-Watson:                   2.023\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              145.751\n",
      "Skew:                          -0.629   Prob(JB):                     2.24e-32\n",
      "Kurtosis:                       5.905   Cond. No.                     7.15e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "[2] The smallest eigenvalue is 1.71e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "Interpretation Hint:\n",
      " - The coefficient for 'njafter' is the DiD estimate.\n",
      " - The constant ('Intercept') represents the average change in 'fte' for the control group (PA).\n",
      " - 'nj' might be dropped or have high standard errors if collinear.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jingx\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 2, but rank is 1\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "# --- Regression in Differences (using dfte) ---\n",
    "\n",
    "if df is not None:\n",
    "    # The 'dfte' variable represents the difference (After - Before) for each store.\n",
    "    # It's typically calculated once per store. Let's use the 'after' period data, \n",
    "    # assuming 'dfte' is populated correctly there.\n",
    "    df_diff_reg_data = df[df['after'] == 1].copy()\n",
    "    \n",
    "    # Check for missing 'dfte' values in this subset\n",
    "    missing_dfte = df_diff_reg_data['dfte'].isnull().sum()\n",
    "    if missing_dfte > 0:\n",
    "        print(f\"Warning: Found {missing_dfte} missing values in 'dfte' for the 'after' period. Dropping these rows for the regression.\")\n",
    "        df_diff_reg_data.dropna(subset=['dfte'], inplace=True)\n",
    "        \n",
    "    print(f\"\\nNumber of observations for difference regression: {len(df_diff_reg_data)}\")\n",
    "\n",
    "    # --- OLS Regression ---\n",
    "    # Formula: dfte ~ nj + njafter \n",
    "    # The constant represents the change for the control group (PA).\n",
    "    # The coefficient on 'nj' would represent the baseline difference in *changes* IF nj varied within this subset (it doesn't independently of njafter here).\n",
    "    # The coefficient on 'njafter' is the DiD estimate (additional change for NJ relative to PA's change).\n",
    "    # Often, just 'dfte ~ njafter' is sufficient if 'nj' becomes collinear with the constant and 'njafter' in this specific setup. \n",
    "    # Let's use a formula that reflects the typical interpretation:\n",
    "    formula_diff = 'dfte ~ njafter + nj' \n",
    "    \n",
    "    try:\n",
    "        model_diff = smf.ols(formula_diff, data=df_diff_reg_data)\n",
    "        results_diff = model_diff.fit()\n",
    "        print(\"\\n--- Regression in Differences (OLS) ---\")\n",
    "        print(results_diff.summary())\n",
    "        \n",
    "        # --- Regression with Robust Standard Errors ---\n",
    "        # Use HC1 type for standard errors robust to heteroskedasticity\n",
    "        results_diff_robust = model_diff.fit(cov_type='HC1') \n",
    "        print(\"\\n--- Regression in Differences (Robust SE - HC1) ---\")\n",
    "        print(results_diff_robust.summary())\n",
    "\n",
    "        print(\"\\nInterpretation Hint:\")\n",
    "        print(\" - The coefficient for 'njafter' is the DiD estimate.\")\n",
    "        print(\" - The constant ('Intercept') represents the average change in 'fte' for the control group (PA).\")\n",
    "        print(\" - 'nj' might be dropped or have high standard errors if collinear.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during the difference regression: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Question 2 as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Estimate the model in levels\n",
    "\n",
    "**Instructions:**\n",
    "Now, we estimate the standard DiD regression using the full dataset (both before and after periods). The dependent variable is the level of employment (`fte`). We include dummy variables for the treatment group (`nj`), the post-treatment period (`after`), and their interaction (`njafter`).\n",
    "\n",
    "The model is: `fte = β₀ + β₁*nj + β₂*after + β₃*njafter + ε`\n",
    "\n",
    "*   `β₀` is the average `fte` for the control group (PA) before the change.\n",
    "*   `β₁` is the average difference in `fte` between NJ and PA before the change.\n",
    "*   `β₂` is the average change in `fte` for the control group (PA) from before to after.\n",
    "*   `β₃` is the DiD estimate – the additional change in `fte` for the treatment group (NJ) relative to the control group's change.\n",
    "\n",
    "We will estimate this using OLS with standard errors and then with robust standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regression in Levels (OLS) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    fte   R-squared:                       0.009\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     2.088\n",
      "Date:                Tue, 01 Apr 2025   Prob (F-statistic):              0.100\n",
      "Time:                        17:44:07   Log-Likelihood:                -2519.3\n",
      "No. Observations:                 698   AIC:                             5047.\n",
      "Df Residuals:                     694   BIC:                             5065.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     20.3000      1.112     18.258      0.000      18.117      22.483\n",
      "nj            -2.9989      1.233     -2.433      0.015      -5.419      -0.579\n",
      "after         -2.0462      1.572     -1.301      0.194      -5.133       1.041\n",
      "njafter        2.3287      1.743      1.336      0.182      -1.094       5.751\n",
      "==============================================================================\n",
      "Omnibus:                      235.266   Durbin-Watson:                   1.353\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1038.719\n",
      "Skew:                           1.489   Prob(JB):                    2.79e-226\n",
      "Kurtosis:                       8.181   Cond. No.                         11.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "--- Regression in Levels (Robust SE - HC1) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    fte   R-squared:                       0.009\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     1.315\n",
      "Date:                Tue, 01 Apr 2025   Prob (F-statistic):              0.268\n",
      "Time:                        17:44:07   Log-Likelihood:                -2519.3\n",
      "No. Observations:                 698   AIC:                             5047.\n",
      "Df Residuals:                     694   BIC:                             5065.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     20.3000      1.502     13.519      0.000      17.357      23.243\n",
      "nj            -2.9989      1.591     -1.884      0.060      -6.118       0.120\n",
      "after         -2.0462      1.789     -1.144      0.253      -5.552       1.460\n",
      "njafter        2.3287      1.931      1.206      0.228      -1.455       6.113\n",
      "==============================================================================\n",
      "Omnibus:                      235.266   Durbin-Watson:                   1.353\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1038.719\n",
      "Skew:                           1.489   Prob(JB):                    2.79e-226\n",
      "Kurtosis:                       8.181   Cond. No.                         11.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "\n",
      "Interpretation Hint:\n",
      " - The coefficient for 'njafter' is the DiD estimate.\n",
      " - 'Intercept': Mean fte for PA (nj=0) Before (after=0).\n",
      " - 'nj': Mean difference NJ - PA Before.\n",
      " - 'after': Mean change After - Before for PA.\n"
     ]
    }
   ],
   "source": [
    "# --- Regression in Levels (using fte) ---\n",
    "\n",
    "if df is not None:\n",
    "    # --- OLS Regression ---\n",
    "    formula_levels = 'fte ~ nj + after + njafter'\n",
    "    model_levels = smf.ols(formula_levels, data=df)\n",
    "    results_levels = model_levels.fit()\n",
    "\n",
    "    print(\"\\n--- Regression in Levels (OLS) ---\")\n",
    "    print(results_levels.summary())\n",
    "\n",
    "    # --- Regression with Robust Standard Errors ---\n",
    "    results_levels_robust = model_levels.fit(cov_type='HC1') # Use HC1 robust SEs\n",
    "    print(\"\\n--- Regression in Levels (Robust SE - HC1) ---\")\n",
    "    print(results_levels_robust.summary())\n",
    "\n",
    "    print(\"\\nInterpretation Hint:\")\n",
    "    print(\" - The coefficient for 'njafter' is the DiD estimate.\")\n",
    "    print(\" - 'Intercept': Mean fte for PA (nj=0) Before (after=0).\")\n",
    "    print(\" - 'nj': Mean difference NJ - PA Before.\")\n",
    "    print(\" - 'after': Mean change After - Before for PA.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Question 3 as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4: Estimate the levels model with clustered standard errors\n",
    "\n",
    "**Instructions:**\n",
    "Observations within the same restaurant (`sheet`) might be correlated over time (e.g., a restaurant that started with high employment might tend to stay high). Standard OLS assumes independent observations, which might not hold here. To account for this potential within-restaurant correlation, we re-estimate the levels model from Question 3, but this time we cluster the standard errors by `sheet`.\n",
    "\n",
    "Compare the standard errors obtained here (especially for the `njafter` coefficient) with those from the OLS and robust estimations in Question 3. Does clustering increase or decrease the standard errors in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regression in Levels (Clustered SE by sheet) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    fte   R-squared:                       0.009\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     1.223\n",
      "Date:                Tue, 01 Apr 2025   Prob (F-statistic):              0.301\n",
      "Time:                        17:44:07   Log-Likelihood:                -2519.3\n",
      "No. Observations:                 698   AIC:                             5047.\n",
      "Df Residuals:                     694   BIC:                             5065.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     20.3000      1.503     13.510      0.000      17.355      23.245\n",
      "nj            -2.9989      1.593     -1.883      0.060      -6.120       0.122\n",
      "after         -2.0462      1.396     -1.466      0.143      -4.782       0.690\n",
      "njafter        2.3287      1.471      1.583      0.114      -0.555       5.213\n",
      "==============================================================================\n",
      "Omnibus:                      235.266   Durbin-Watson:                   1.353\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1038.719\n",
      "Skew:                           1.489   Prob(JB):                    2.79e-226\n",
      "Kurtosis:                       8.181   Cond. No.                         11.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n",
      "\n",
      "--- Standard Error Comparison for 'njafter' Coefficient ---\n",
      "OLS Standard Error:          1.7431\n",
      "Robust SE (HC1):           1.9308\n",
      "Clustered SE (by sheet):   1.4715\n",
      "\n",
      "Observation: Clustering standard errors decreased the SE for 'njafter' compared to OLS and standard Robust SE.\n"
     ]
    }
   ],
   "source": [
    "# --- Regression in Levels with Clustered Standard Errors ---\n",
    "\n",
    "if df is not None and results_levels is not None and results_levels_robust is not None:\n",
    "    # Ensure the clustering variable 'sheet' doesn't have missing values \n",
    "    # in the data used by the model (OLS uses listwise deletion by default)\n",
    "    valid_indices = results_levels.model.data.row_labels # Get indices used in the fitted model\n",
    "    cluster_groups = df.loc[valid_indices, 'sheet']\n",
    "\n",
    "    # Estimate the model with standard errors clustered by 'sheet'\n",
    "    results_levels_clustered = model_levels.fit(cov_type='cluster', \n",
    "                                                cov_kwds={'groups': cluster_groups})\n",
    "\n",
    "    print(\"\\n--- Regression in Levels (Clustered SE by sheet) ---\")\n",
    "    print(results_levels_clustered.summary())\n",
    "\n",
    "    # --- Comparison of Standard Errors ---\n",
    "    print(\"\\n--- Standard Error Comparison for 'njafter' Coefficient ---\")\n",
    "    try:\n",
    "        ols_se = results_levels.bse['njafter']\n",
    "        robust_se = results_levels_robust.bse['njafter']\n",
    "        clustered_se = results_levels_clustered.bse['njafter']\n",
    "        \n",
    "        print(f\"OLS Standard Error:          {ols_se:.4f}\")\n",
    "        print(f\"Robust SE (HC1):           {robust_se:.4f}\")\n",
    "        print(f\"Clustered SE (by sheet):   {clustered_se:.4f}\")\n",
    "\n",
    "        if clustered_se < ols_se and clustered_se < robust_se:\n",
    "             print(\"\\nObservation: Clustering standard errors decreased the SE for 'njafter' compared to OLS and standard Robust SE.\")\n",
    "        elif clustered_se > ols_se or clustered_se > robust_se:\n",
    "             print(\"\\nObservation: Clustering standard errors increased the SE for 'njafter' compared to OLS and/or standard Robust SE.\")\n",
    "        else:\n",
    "             print(\"\\nObservation: Clustering standard errors resulted in a similar SE for 'njafter' compared to OLS / Robust SE.\")\n",
    "             \n",
    "    except KeyError:\n",
    "        print(\"\\nCould not retrieve standard errors for comparison (perhaps model estimation failed).\")\n",
    "    except NameError:\n",
    "        print(\"\\nCould not retrieve standard errors for comparison (previous model results not available).\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping Question 4 as data or previous results were not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 5: Estimate the levels model using fixed effects\n",
    "\n",
    "**Instructions:**\n",
    "Fixed effects regression controls for all time-invariant characteristics of each restaurant (e.g., location, management quality, initial size category). We use `sheet` as the entity identifier for the fixed effects. This is often implemented using the \"within\" estimator.\n",
    "\n",
    "Estimate the model `fte ~ nj + after + njafter` including restaurant fixed effects. Observe which variables are estimated and which, if any, are dropped. Explain *why* any variables are dropped. We will use the `PanelOLS` estimator from the `linearmodels` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting PanelOLS estimation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fixed Effects (PanelOLS) Regression ---\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                    fte   R-squared:                        0.0114\n",
      "Estimator:                   PanelOLS   R-squared (Between):             -0.0094\n",
      "No. Observations:                 698   R-squared (Within):               0.0114\n",
      "Date:                Tue, Apr 01 2025   R-squared (Overall):             -0.0084\n",
      "Time:                        17:44:07   Log-likelihood                   -2004.1\n",
      "Cov. Estimator:            Unadjusted                                           \n",
      "                                        F-statistic:                      2.0070\n",
      "Entities:                         349   P-value                           0.1359\n",
      "Avg Obs:                       2.0000   Distribution:                   F(2,347)\n",
      "Min Obs:                       2.0000                                           \n",
      "Max Obs:                       2.0000   F-statistic (robust):             2.0070\n",
      "                                        P-value                           0.1359\n",
      "Time periods:                       2   Distribution:                   F(2,347)\n",
      "Avg Obs:                       349.00                                           \n",
      "Min Obs:                       349.00                                           \n",
      "Max Obs:                       349.00                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "njafter        2.3287     1.1784     1.9762     0.0489      0.0111      4.6464\n",
      "after_col     -2.0462     1.0630    -1.9249     0.0551     -4.1369      0.0446\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 3.4038\n",
      "P-value: 0.0000\n",
      "Distribution: F(348,347)\n",
      "\n",
      "Included effects: Entity\n",
      "\n",
      "Variables included in final FE model: ['njafter', 'after_col']\n",
      "Note: 'nj' was likely dropped (absorbed by EntityEffects). The constant is also absorbed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jingx\\AppData\\Local\\Temp\\ipykernel_32340\\3729254006.py:37: AbsorbingEffectWarning: \n",
      "Variables have been fully absorbed and have removed from the regression:\n",
      "\n",
      "nj\n",
      "\n",
      "  results_fe = mod_fe.fit()\n"
     ]
    }
   ],
   "source": [
    "# --- Fixed Effects (Within) Regression using PanelOLS ---\n",
    "\n",
    "if df is not None:\n",
    "    try:\n",
    "        # Prepare data for PanelOLS: Set BOTH entity ('sheet') and time ('after') as the index.\n",
    "        df_panel = df.copy()\n",
    "        \n",
    "        # Define key columns for checking NaNs and potential dropping\n",
    "        key_cols = ['sheet', 'after', 'fte', 'nj', 'njafter']\n",
    "        initial_rows = len(df_panel)\n",
    "        \n",
    "        # Check for and handle missing values in key columns before setting index\n",
    "        if df_panel[key_cols].isnull().any().any():\n",
    "             print(f\"Warning: Data contains NaNs in key columns {key_cols}. Dropping rows with NaNs in these columns.\")\n",
    "             df_panel.dropna(subset=key_cols, inplace=True)\n",
    "             print(f\"Dropped {initial_rows - len(df_panel)} rows due to NaNs.\")\n",
    "        \n",
    "        # Ensure 'sheet' and 'after' are suitable types for index (e.g., int or category)\n",
    "        df_panel['sheet'] = df_panel['sheet'].astype(int)\n",
    "        df_panel['after'] = df_panel['after'].astype(int)\n",
    "\n",
    "        # Set the MultiIndex\n",
    "        df_panel = df_panel.set_index(['sheet', 'after'])\n",
    "\n",
    "        # *** Workaround: Create a duplicate 'after' column for use in the formula ***\n",
    "        # This column is needed because set_index removes 'after' from the regular columns.\n",
    "        df_panel['after_col'] = df_panel.index.get_level_values('after')\n",
    "\n",
    "        # Define and estimate the model including entity (restaurant) fixed effects\n",
    "        # Formula uses 'after_col' instead of 'after'. \n",
    "        # 'nj' is included to explicitly see it get dropped.\n",
    "        # Add 'EntityEffects' to the formula.\n",
    "        formula_fe = 'fte ~ njafter + after_col + nj + EntityEffects' \n",
    "        \n",
    "        print(\"\\nAttempting PanelOLS estimation...\")\n",
    "        mod_fe = PanelOLS.from_formula(formula_fe, data=df_panel, drop_absorbed=True)\n",
    "        results_fe = mod_fe.fit()\n",
    "\n",
    "        print(\"\\n--- Fixed Effects (PanelOLS) Regression ---\")\n",
    "        print(results_fe)\n",
    "        \n",
    "        # Explicitly check which variables were dropped (absorbed)\n",
    "        print(\"\\nVariables included in final FE model:\", results_fe.params.index.tolist())\n",
    "        print(\"Note: 'nj' was likely dropped (absorbed by EntityEffects). The constant is also absorbed.\")\n",
    "\n",
    "\n",
    "    except pd.errors.DuplicateLabelError as e:\n",
    "         print(f\"\\nError: Duplicate index entries found. Check if any sheet/after combination appears more than once. {e}\")\n",
    "         results_fe = None\n",
    "    except KeyError as e:\n",
    "        print(f\"\\nAn error occurred during Fixed Effects estimation: A required column is missing - {e}\")\n",
    "        results_fe = None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during the Fixed Effects estimation: {e}\")\n",
    "        print(\"This might happen if the data structure is not correctly recognized as a panel.\")\n",
    "        results_fe = None \n",
    "\n",
    "else:\n",
    "    print(\"Skipping Question 5 as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Explanation of Dropped Variables (Fixed Effects) ---\n",
    "\n",
    "**Variable(s) Dropped:**\n",
    "1.  `nj`: The variable indicating whether a restaurant is in New Jersey.\n",
    "2.  The overall constant (Intercept).\n",
    "\n",
    "**Reason:**\n",
    "*   **`nj`:** As explained before, the fixed effects estimator controls for all time-invariant characteristics specific to each restaurant (`sheet`). Since a restaurant's location (`nj`) doesn't change over time, its effect is perfectly collinear with the restaurant's unique fixed effect. It cannot be estimated separately and is absorbed/dropped.\n",
    "*   **Constant:** The entity fixed effects (one for each restaurant, minus a reference category) act like a set of dummy variables. This set of dummies is perfectly collinear with an overall constant term. Therefore, the constant is typically dropped when entity fixed effects are included to avoid multicollinearity (the dummy variable trap). The baseline level for each entity is captured by its specific fixed effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
